{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stream processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Message queue"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section we will introduce Apache Kafka ( https://kafka.apache.org/ ). Kafka is a distributed message broker were you can subscribe for a topic and receive a stream of messages. You will also see how to publish messages. Let's start with the initial setup.\n",
    "\n",
    "You will be using the pykafka library ( http://pykafka.readthedocs.io/en/latest/ ). It should be already installed on the ICCluster, but you may need to install it locally if you work on your own computer. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pykafka import KafkaClient\n",
    "import getpass\n",
    "\n",
    "ZOOKEEPER_QUORUM = 'iccluster042.iccluster.epfl.ch:2181,iccluster054.iccluster.epfl.ch:2181,iccluster078.iccluster.epfl.ch:2181'\n",
    "\n",
    "username = getpass.getuser()\n",
    "\n",
    "client = KafkaClient(zookeeper_hosts=ZOOKEEPER_QUORUM)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1.a** Look at the pykafka documentation and list the topics currently available:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#TODO: print the topics\n",
    "client.<FILL>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see there are already some topics there and unfortunately no separation per user. So, be respectful with the work of your colleagues and with the infrastructure we put in place. More specifically, never write in a topic that you didn't create yourself or your teammate (if not expressly asked to). Be also carefull not to create infinite loops by publishing in the same topic you subscribed, without filtering or exit conditions.\n",
    "\n",
    "**1.b** That being said, let's create our own topic. Don't forget to make it unique with your username."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "name = ('test-topic-' + username).encode()\n",
    "#TODO: create topic and get a handle\n",
    "tt=<FILL>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Kafka is configured to autocreate topics if they don't exist a priori. You can check in the list of topics, it should be there now. \n",
    "\n",
    "**1.c** To write a message to the topic you need to instanciate a Producer. For simplicity and as we won't deal with very large (or very fast) data, you can use a synchronous publisher. Then, send a series of 10 different messages (for example \"Hello world {i}!\" with i from 1 to 10). You might need to encode the string to utf-8 before sending:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with tt.get_sync_producer() as producer:\n",
    "    for i in range(10):\n",
    "        #TODO produce Hello world i"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1.d** Now let's check what is in our topic and subscribe to it. Again, for simplicity reason, you can use the basic ```get_simple_consumer```. Notice that the reading loop never finishes, streams are infinite! You may want to catch the KeyboardInterrupt to avoid polluting the output with a stacktrace:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#TODO: get the consumer\n",
    "consumer = tt.<FILL>\n",
    "\n",
    "try:\n",
    "    #TODO: get message and print it if not null\n",
    "    <FILL>\n",
    "except KeyboardInterrupt:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you run the previous cell several times you will notice that it always restarts from the beginning by default. We have configured Kafka to persist the data for a quite long time. So be also careful not to leave data producers running uselessly.\n",
    "\n",
    "This is useful for testing and re-running the same pipeline several time. But in production setting, you may want to remember the last message seen and continue from that. Luckily, Kafka can do it for you if you define a consumer group and commit the current offset (see the documentation: http://pykafka.readthedocs.io/en/latest/usage.html#consumer-patterns ). You can either set the auto_commit_enable, or manually call the commit_offsets method.\n",
    "\n",
    "**1.e** Rewrite the above consummer loop to only show the newest elements every-time it is run. You can modify and re-run the producing loop to add some more messages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pykafka.common import OffsetType\n",
    "\n",
    "#TODO: get the consumer at latest offset, use a unique name for the consumer group\n",
    "consumer_group_name=('test-group-' + username).encode()\n",
    "consumer = tt.<FILL>\n",
    "\n",
    "try:\n",
    "    #TODO: get message and print it if not null\n",
    "    <FILL>\n",
    "except KeyboardInterrupt:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "We will now generate a little bit more data and see how to make a live plot. We can use another topic, like `b\"test-random-<insert you username>\"`.\n",
    "\n",
    "**1.f** First write an infinite generator (or just a for loop) that outputs the 2D coordinates of a random walk (here you can simply take a random walk on a grid, by taking a random jump of length 1 in any of the 4 directions at each step). Don't forget to add a `time.sleep(1)` each time after you publish it to Kafka."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tr = client.topics[('test-random-' + username).encode()]\n",
    "\n",
    "#TODO: import math dependencies\n",
    "\n",
    "#TODO: create a random walk generator\n",
    "def <FILL>:\n",
    "    \n",
    "#TODO: fill the missing parts to produce messages for the random generated walk.\n",
    "with tr.get_sync_producer() as producer:\n",
    "    try:\n",
    "        for p in <FILL>:\n",
    "            <FILL>\n",
    "            time.sleep(0.1)\n",
    "    except KeyboardInterrupt:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1.g** Now we will use the library bokeh ( https://bokeh.pydata.org/en/latest/docs/user_guide.html ) for plotting. It allows for live update of the graph on the notebook, pushing data from the kernel. First you need to create a line plot with a ColumnDataSource initialized with empty lists for x and y. Then, in the consuming loop, you will parse the message and update the data of the source. Make sure your code would work for a long running job without eating all the memory ;-)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from bokeh.io import push_notebook, show, output_notebook\n",
    "from bokeh.plotting import figure\n",
    "from bokeh.models import ColumnDataSource, Range1d\n",
    "import time\n",
    "\n",
    "output_notebook()\n",
    "\n",
    "x=[]\n",
    "y=[]\n",
    "source = ColumnDataSource(data=dict(x=x, y=y))\n",
    "\n",
    "p = figure(plot_width=850,plot_height=450)\n",
    "p.line('x', 'y', source=source)\n",
    "\n",
    "p.x_range = Range1d(-50, 50)\n",
    "p.y_range = Range1d(-50, 50)\n",
    "\n",
    "handle=show(p, notebook_handle=True)\n",
    "\n",
    "#TODO: get a handle on the consumer from test-random-<username>\n",
    "consumer = <FILL>\n",
    "\n",
    "#TODO: fill missing parts to get the X,Y pairs, and plot the last 20 values.\n",
    "try:\n",
    "    for message in consumer:\n",
    "        if message is not None:\n",
    "            \n",
    "            <FILL>\n",
    "            \n",
    "            source.data = dict(x=x, y=y)\n",
    "            push_notebook(handle=handle)\n",
    "            time.sleep(0.1)\n",
    "except KeyboardInterrupt:\n",
    "    pass\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that in a notebook you can only run one cell at the time. You thus must stop the producer before you can run the consumer. If you want to play with the producer and the consumer at the same time, feel free to copy one of them to another notebook or in a python script to be able to run both of them simultaneously. Do not forget to include the cells needed initialize the kafka client as well. If you are working with a teammate (e.g. one is producer and the other is the consumer), make sure that you agree on using the same topic name."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Spark streaming"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the last module, you have experimented with Spark, processing batches of data in RDDs and dataframes. Spark also supports \"streaming\", namely it defines a stream as a sequence of mini-batches. You can find a pretty detailed introduction here: https://spark.apache.org/docs/2.3.2/streaming-programming-guide.html. Pay particular attention to the section [_points to remember_](https://spark.apache.org/docs/2.3.2/streaming-programming-guide.html#points-to-remember)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "import os\n",
    "\n",
    "os.environ['PYSPARK_PYTHON'] = '/opt/anaconda3/bin/python'\n",
    "\n",
    "conf = pyspark.conf.SparkConf()\n",
    "conf.setMaster('yarn')\n",
    "conf.setAppName('streaming-{0}'.format(username))\n",
    "conf.set('spark.executor.memory', '1g')\n",
    "conf.set('spark.executor.instances', '1')\n",
    "conf.set('spark.executor.cores', '2') # must be greater than the number of sources\n",
    "conf.set('spark.port.maxRetries', '100')\n",
    "#conf.set('spark.jars.packages', 'org.apache.spark:spark-streaming-kafka-0-10_2.11:2.3.2')\n",
    "conf.set('spark.jars.packages', 'org.apache.spark:spark-streaming-kafka-0-8_2.11:2.3.2')\n",
    "sc = pyspark.SparkContext.getOrCreate(conf)\n",
    "conf = sc.getConf()\n",
    "sc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark import SparkContext\n",
    "from pyspark.streaming import StreamingContext\n",
    "\n",
    "# Create a StreamingContext with two working thread and batch interval of 2 second.\n",
    "# Each time you stop a StreamingContext, you will need to recreate it here.\n",
    "ssc = StreamingContext(sc, 2)\n",
    "ssc.checkpoint('hdfs:///homes/{}/checkpoint/'.format(username))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can open the Spark UI, following the link in the output above and you should notice a new tab (only when a stream is running), showing the processing of each mini-batch."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 From a RDD queue"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will start with a very simple stream, out of a list of RDDs generated manually. Here is an example of RDDs with a quite simple counter. \n",
    "\n",
    "```\n",
    "rdds = []\n",
    "for i in range(20):\n",
    "    rdds.append(sc.range(i * 100, (i + 1) * 100))\n",
    "\n",
    "```\n",
    "\n",
    "**2.1.a** In the same vein, generate a list of RDDs with random numbers with a uniform distribution. (hint: have a look at the [spark MLlib package](https://spark.apache.org/docs/2.3.2/api/python/pyspark.mllib.html))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "rdds = []\n",
    "# TODO: create the RDD of random values in range [0,1)\n",
    "<FILL>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's create the stream and print it from the stream processor. The python API for spark streaming has only a few options to get your results back. `pprint`is the most convenient for debugging and testing, `saveAsTextFiles` can be used for persisting the output and the most generic one is `foreachRDD` which can execute any function. ( https://spark.apache.org/docs/latest/streaming-programming-guide.html#output-operations-on-dstreams )\n",
    "\n",
    "Once you start the stream, you can check on the UI what is happening and you should see the output here after each batch. You will need to explicitely call stop, which requires you to re-instanciate the streaming context if you want to start a new stream, or re-start this one.\n",
    "\n",
    "**2.1.b** Run the stream and don't forget to stop it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dstream = ssc.queueStream(rdds)\n",
    "\n",
    "# Print the first ten elements of each RDD generated in this DStream to the console\n",
    "dstream.pprint()\n",
    "ssc.start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "ssc.stop(stopSparkContext=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's get back to our random walk example. This demonstrate the capacity of Spark streaming to maintain a current state and apply updates to it. The state is backuped in your user folder on hdfs (as set while creating the streaming context) and you could pass this when recreating the context to re-start from the point when you stopped it.\n",
    "\n",
    "Do not forget to stop your StreamContext (with `stopSparkContext=False`) when you are done. And remember to recreate the StreamContext after you stop it before reusing it.\n",
    "\n",
    "**2.1.c** Apply some transformations on the stream of random data such that it outputs 2D coordinates of a random walk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Recreate the streaming context if it has been stopped\n",
    "ssc = StreamingContext(sc, 2)\n",
    "ssc.checkpoint('hdfs:///homes/{}/checkpoint/'.format(username))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "def walk(randValue, xyPosition):\n",
    "    if xyPosition is None:\n",
    "        xyPosition = (0,0)\n",
    "    if randValue:\n",
    "        # TODO: randomly move xyPosition to one of 4 directions and return it\n",
    "        <FILL>\n",
    "    else\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dstream = ssc.queueStream(rdds)\n",
    "dstream = dstream.map(lambda x: (1, x)).updateStateByKey(walk)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2.1.d** Modify the following output function such that it produces data that can then by plotted by the same function you wrote before in the previous section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pykafka import KafkaClient\n",
    "\n",
    "def sendToKafka(iter):\n",
    "    client = KafkaClient(zookeeper_hosts=ZOOKEEPER_QUORUM)\n",
    "    with client.topics[('test-random-' + username).encode()].get_sync_producer() as producer:\n",
    "        for record in iter:\n",
    "            # TODO\n",
    "            producer.produce(<FILL>)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dstream.map(lambda x: len(x[1])).foreachRDD(lambda rdd: rdd.foreachPartition(sendToKafka))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2.1.e** Now you can start it and while it is running launch the plot cell too. Note that it will stop producing events after all the elements of rdds have been processed. You can stop the StreamContext at that point."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "ssc.start()  # Start the computation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ssc.stop(stopSparkContext=False) # Don't forget to stop the stream when you are done"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 From Kafka"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we will consume message from the `wiki-edits` topic. It contains the stream of events fetched from the wikimedia sites.\n",
    "More info: https://www.mediawiki.org/wiki/API:Recent_changes_stream\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from  pyspark.streaming.kafka import KafkaUtils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ssc = StreamingContext(spark.sparkContext, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dstream = KafkaUtils.createStream(ssc, ZOOKEEPER_QUORUM, ('test-wiki-group-' + username).encode(), {'wiki-edits': 1})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dstream.pprint()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "ssc.start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "ssc.stop(stopSparkContext=False, stopGraceFully=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prototyping spark transformations on a stream can be cumbersome, so we can also create a static RDD from Kafka to experiment first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.streaming.kafka import OffsetRange"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "wiki_rdd_source = KafkaUtils.createRDD(sc, {'bootstrap.servers': 'iccluster042.iccluster.epfl.ch:6667'}, [OffsetRange('wiki-edits',0,10000,10100)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code above creates a RDD from the `wiki-edits` Kafka topic, that goes from offset `10000` to offset `10100`.\n",
    "\n",
    "Note: we start from the 10000th message, but you can play with different offsets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "wiki_rdd_source.first()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "wiki_rdd_source.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that we got the same type of data  as in the stream above.\n",
    "\n",
    "Now, we can write the function to parse the json messages.\n",
    "\n",
    "Note: sometimes the messages are invalid json, so we will use `flatMap` to discard those."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def parse_wiki(doc):\n",
    "    try:\n",
    "        return [json.loads(doc)]\n",
    "    except json.decoder.JSONDecodeError:\n",
    "        return []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "wiki_rdd = wiki_rdd_source.flatMap(lambda x: parse_wiki(x[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "wiki_rdd.first()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "wiki_rdd.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also use this on the stream:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ssc = StreamingContext(spark.sparkContext, 10)\n",
    "dstream = KafkaUtils.createStream(ssc, ZOOKEEPER_QUORUM, ('test-wiki-group-' + username).encode(), {'wiki-edits': 1})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "wiki_dstream = dstream.flatMap(lambda x: parse_wiki(x[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "wiki_dstream.pprint()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "ssc.start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "ssc.stop(stopSparkContext=False, stopGraceFully=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2.2.a** Create a stream from `wiki-edits` which contains only the `type` and `wiki` fields."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3 Window operations\n",
    "\n",
    "We have seen how to collect data into a stream on spark RDDs and how to apply simple transformations on them. Spark allows to perform window operations on streams.\n",
    "\n",
    "Documentation: https://spark.apache.org/docs/2.3.0/streaming-programming-guide.html#window-operations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's have a look at what we can get from the `wiki-edits`. We will perform operations on windows of size 10 minutes, every 2 seconds.\n",
    "\n",
    "As a simple example, here is how to count the number of messages for such a window."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ssc = StreamingContext(spark.sparkContext, 2)\n",
    "ssc.checkpoint('/tmp/chk')\n",
    "dstream = KafkaUtils.createStream(ssc, ZOOKEEPER_QUORUM, ('test-wiki-group-' + username).encode(), {'wiki-edits': 1})\n",
    "wiki_dstream = dstream.flatMap(lambda x: parse_wiki(x[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 600s = 10minutes, every 2 seconds\n",
    "wiki_counts = wiki_dstream.countByWindow(600, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "wiki_counts.pprint()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "ssc.start()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Look at the documentation to see how to use the window operations.\n",
    "\n",
    "**2.3.a** Create a stream that counts the actions done by a bot and those done by humans within the time window.\n",
    "\n",
    "**2.3.b** Create a stream that gets the number of actions by `type`. Also get the number of actions by `wiki`.\n",
    "\n",
    "**2.3.c** Send the above streams to kafka topics (example in **2.1**) and use that to dynamically plot:\n",
    "- the volume of actions by bots vs actions by humans.\n",
    "- the volume of actions by `type`, by `wiki`,\n",
    "\n",
    "Use `wiki-plot-bot-<username>`,  `wiki-plot-type-<username>` and `wiki-plot-wiki-<username>` for the respective topic names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#TODO 2.3.a: create a stream and count actions by bot vs actions by humans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#TODO 2.3.b: Create stream that gets the number of actions by type. Also get the number of actions by wiki."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#TODO 2.3.c: write the streams to topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#TODO: create the plot in bokeh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#TODO: stop the stream context, and release the spark context to free resources"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
